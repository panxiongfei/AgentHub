"""
FastAPI ä¸»åº”ç”¨æ–‡ä»¶
åŒ…å«åŸºæœ¬çš„ API è·¯ç”±å’Œå¥åº·æ£€æŸ¥
"""

import json
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from app import __version__, __description__
from app.config.settings import get_settings
from app.core.exceptions import AgentHubException
from app.core.logger import get_logger

# è·å–é…ç½®å’Œæ—¥å¿—
settings = get_settings()
logger = get_logger("api")

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="AgentHub",
    description=__description__,
    version=__version__,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.app.cors_origins,
    allow_credentials=settings.app.cors_allow_credentials,
    allow_methods=settings.app.cors_allow_methods,
    allow_headers=settings.app.cors_allow_headers,
)


@app.exception_handler(AgentHubException)
async def agenthub_exception_handler(request, exc: AgentHubException):
    """è‡ªå®šä¹‰å¼‚å¸¸å¤„ç†å™¨"""
    logger.error(
        "AgentHub exception occurred",
        error_code=exc.code,
        error_message=exc.message,
        error_details=exc.details,
        path=str(request.url)
    )
    
    return JSONResponse(
        status_code=400,
        content=exc.to_dict()
    )


@app.exception_handler(Exception)
async def general_exception_handler(request, exc: Exception):
    """é€šç”¨å¼‚å¸¸å¤„ç†å™¨"""
    logger.error(
        "Unexpected exception occurred",
        error_type=type(exc).__name__,
        error_message=str(exc),
        path=str(request.url)
    )
    
    return JSONResponse(
        status_code=500,
        content={
            "error": True,
            "code": "INTERNAL_SERVER_ERROR",
            "message": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯",
            "details": {}
        }
    )


@app.get("/")
async def root() -> Dict[str, Any]:
    """æ ¹è·¯å¾„"""
    return {
        "message": "æ¬¢è¿ä½¿ç”¨ AgentHub",
        "version": __version__,
        "description": __description__,
        "docs_url": "/docs",
        "health_url": "/health"
    }


@app.get("/health")
async def health_check() -> Dict[str, Any]:
    """å¥åº·æ£€æŸ¥"""
    try:
        # åŸºæœ¬å¥åº·çŠ¶æ€
        health_status = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "version": __version__,
            "services": {}
        }
        
        # æ£€æŸ¥å„ä¸ªæœåŠ¡çŠ¶æ€
        # TODO: åœ¨å®ç°å„æœåŠ¡åæ·»åŠ å…·ä½“æ£€æŸ¥
        
        # æ£€æŸ¥é…ç½®
        try:
            _ = get_settings()
            health_status["services"]["config"] = {"status": "healthy", "message": "é…ç½®åŠ è½½æ­£å¸¸"}
        except Exception as e:
            health_status["services"]["config"] = {"status": "unhealthy", "message": f"é…ç½®é”™è¯¯: {str(e)}"}
            health_status["status"] = "unhealthy"
        
        # æ£€æŸ¥æ•°æ®åº“ï¼ˆæš‚æ—¶è·³è¿‡ï¼Œæ•°æ®åº“æ¨¡å—å°šæœªå®ç°ï¼‰
        health_status["services"]["database"] = {"status": "not_implemented", "message": "æ•°æ®åº“æ¨¡å—å°šæœªå®ç°"}
        
        # æ£€æŸ¥è°ƒåº¦å™¨ï¼ˆæš‚æ—¶è·³è¿‡ï¼Œéœ€è¦åœ¨å¯åŠ¨æ—¶åˆå§‹åŒ–ï¼‰
        health_status["services"]["scheduler"] = {"status": "not_implemented", "message": "è°ƒåº¦å™¨æœªå¯åŠ¨"}
        
        logger.info("Health check completed", status=health_status["status"])
        
        return health_status
        
    except Exception as e:
        logger.error("Health check failed", error=str(e))
        return {
            "status": "unhealthy",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }


@app.get("/info")
async def app_info() -> Dict[str, Any]:
    """åº”ç”¨ä¿¡æ¯"""
    return {
        "name": settings.app.name,
        "version": __version__,
        "description": __description__,
        "debug": settings.app.debug,
        "timezone": settings.scheduler.timezone,
        "api_prefix": settings.app.api_prefix,
    }


# API è·¯ç”±ç»„
@app.get(f"{settings.app.api_prefix}/status")
async def api_status() -> Dict[str, Any]:
    """API çŠ¶æ€"""
    return {
        "api_version": "v1",
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "endpoints": {
            "health": "/health",
            "info": "/info",
            "docs": "/docs",
            "redoc": "/redoc"
        }
    }


# ç¤ºä¾‹è·¯ç”±ï¼ˆåç»­ä¼šè¢«å®é™…çš„ä¸šåŠ¡è·¯ç”±æ›¿æ¢ï¼‰
@app.get(f"{settings.app.api_prefix}/platforms")
async def list_platforms() -> Dict[str, Any]:
    """åˆ—å‡ºæ”¯æŒçš„å¹³å°"""
    return {
        "platforms": [
            {
                "name": "manus",
                "display_name": "Manus",
                "enabled": True,
                "status": "active"
            },
            {
                "name": "skywork", 
                "display_name": "Skywork",
                "enabled": True,
                "status": "active"
            },
            {
                "name": "coze_space",
                "display_name": "æ‰£å­ç©ºé—´",
                "enabled": True,
                "status": "active"
            }
        ]
    }


@app.get(f"{settings.app.api_prefix}/tasks")
async def list_tasks() -> Dict[str, Any]:
    """åˆ—å‡ºä»»åŠ¡"""
    # TODO: å®ç°ä»»åŠ¡åˆ—è¡¨è·å–
    return {
        "tasks": [],
        "total": 0,
        "message": "ä»»åŠ¡ç®¡ç†åŠŸèƒ½å°šæœªå®ç°"
    }


@app.get(f"{settings.app.api_prefix}/topics") 
async def list_topics() -> Dict[str, Any]:
    """åˆ—å‡ºå‘½é¢˜"""
    # TODO: å®ç°å‘½é¢˜åˆ—è¡¨è·å–
    return {
        "topics": [],
        "total": 0,
        "message": "å‘½é¢˜ç®¡ç†åŠŸèƒ½å°šæœªå®ç°"
    }


@app.get(f"{settings.app.api_prefix}/system/info")
async def system_info() -> Dict[str, Any]:
    """ç³»ç»Ÿä¿¡æ¯"""
    import psutil
    import platform
    from datetime import datetime
    
    try:
        # è·å–ç³»ç»ŸåŸºæœ¬ä¿¡æ¯
        system_info = {
            "timestamp": datetime.now().isoformat(),
            "system": {
                "platform": platform.system(),
                "platform_release": platform.release(),
                "platform_version": platform.version(),
                "architecture": platform.machine(),
                "hostname": platform.node(),
                "processor": platform.processor(),
                "python_version": platform.python_version()
            },
            "resources": {
                "cpu_count": psutil.cpu_count(),
                "cpu_percent": psutil.cpu_percent(interval=1),
                "memory": {
                    "total": psutil.virtual_memory().total,
                    "available": psutil.virtual_memory().available,
                    "percent": psutil.virtual_memory().percent
                },
                "disk": {
                    "total": psutil.disk_usage('/').total,
                    "free": psutil.disk_usage('/').free,
                    "percent": psutil.disk_usage('/').percent
                }
            },
            "application": {
                "name": settings.app.name,
                "version": __version__,
                "debug": settings.app.debug,
                "api_prefix": settings.app.api_prefix
            }
        }
        
        return system_info
        
    except Exception as e:
        logger.error("Failed to get system info", error=str(e))
        return {
            "error": True,
            "message": f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }


# å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    logger.info(
        "AgentHub API starting up",
        version=__version__,
        debug=settings.app.debug,
        api_prefix=settings.app.api_prefix
    )


# å…³é—­äº‹ä»¶  
@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    logger.info("AgentHub API shutting down")


# å†å²ä»»åŠ¡ç›¸å…³ç«¯ç‚¹
@app.get(f"{settings.app.api_prefix}/history")
async def list_history_tasks(
    platform: Optional[str] = None,
    status: Optional[str] = None,
    page: int = 1,
    size: int = 20
) -> Dict[str, Any]:
    """åˆ—å‡ºå†å²ä»»åŠ¡"""
    try:
        history_tasks = []
        stats = {"total": 0, "successful": 0, "failed": 0, "file_count": 0}
        
        # æ‰«æå†å²ä¸‹è½½ç›®å½•
        base_download_dir = Path("data/history_downloads")
        multi_platform_dir = Path("data/multi_platform_downloads")
        
        # ğŸ”¥ æ–°å¢ï¼šæ‰«æå„å¹³å°ç‰¹å®šçš„ä¸‹è½½ç›®å½•
        platform_specific_dirs = {
            "skywork": ["data/skywork_history", "data/skywork_downloads"],
            "manus": ["data/manus_history", "data/manus_downloads"],
            "coze_space": ["data/coze_space_history_downloads", "data/coze_downloads"]
        }
        
        # æ‰«æå¤šå¹³å°ä¸‹è½½ç›®å½•
        if multi_platform_dir.exists():
            for session_dir in multi_platform_dir.glob("multi_platform_history_*"):
                for platform_dir in session_dir.iterdir():
                    if platform_dir.is_dir():
                        # æ‰«æå¹³å°ç›®å½•ä¸‹çš„æ‰€æœ‰ä»»åŠ¡ç›®å½•
                        for task_dir in platform_dir.glob("task_*"):
                            if task_dir.is_dir():
                                task_data = await _load_task_data(task_dir, platform_dir.name)
                                if task_data:
                                    # åº”ç”¨çŠ¶æ€è¿‡æ»¤
                                    if status and task_data.get("success") != (status == "success"):
                                        continue
                                    history_tasks.append(task_data)
        
        # ğŸ”¥ æ‰«æå„å¹³å°ç‰¹å®šçš„ä¸‹è½½ç›®å½•
        for platform_name, dirs in platform_specific_dirs.items():
            # å¦‚æœæŒ‡å®šäº†å¹³å°è¿‡æ»¤ï¼Œè·³è¿‡å…¶ä»–å¹³å°
            if platform and platform != platform_name:
                continue
                
            for dir_path in dirs:
                platform_dir = Path(dir_path)
                if platform_dir.exists():
                    # æ‰«æä¸‹è½½ä¼šè¯ç›®å½•ï¼ˆå¦‚ quick_xxx, batch_xxx ç­‰ï¼‰
                    for session_dir in platform_dir.iterdir():
                        if session_dir.is_dir():
                            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸‹è½½æŠ¥å‘Šæ–‡ä»¶æ‰€åœ¨ç›®å½•
                            download_report = session_dir / "download_report.json"
                            if download_report.exists():
                                # ä»ä¸‹è½½æŠ¥å‘Šä¸­åŠ è½½ä»»åŠ¡
                                tasks_from_report = await _load_tasks_from_download_report(download_report, platform_name)
                                for task_data in tasks_from_report:
                                    if status and task_data.get("success") != (status == "success"):
                                        continue
                                    history_tasks.append(task_data)
                            else:
                                # ç›´æ¥æ‰«æä»»åŠ¡ç›®å½•
                                for task_dir in session_dir.glob("task_*"):
                                    if task_dir.is_dir():
                                        task_data = await _load_task_data(task_dir, platform_name)
                                        if task_data:
                                            if status and task_data.get("success") != (status == "success"):
                                                continue
                                            history_tasks.append(task_data)
        
        # å•å¹³å°ä¸‹è½½ç›®å½•
        if base_download_dir.exists():
            for task_dir in base_download_dir.glob("task_*"):
                if task_dir.is_dir():
                    # ğŸ”¥ ä¿®å¤ï¼šä»metadataä¸­è¯»å–æ­£ç¡®çš„å¹³å°ä¿¡æ¯
                    detected_platform = await _detect_platform_from_metadata(task_dir)
                    task_data = await _load_task_data(task_dir, detected_platform)
                    if task_data:
                        if platform and task_data.get("platform") != platform:
                            continue
                        if status and task_data.get("success") != (status == "success"):
                            continue
                        history_tasks.append(task_data)
        
        # å»é‡å¤„ç†
        history_tasks = _deduplicate_tasks(history_tasks)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        for task in history_tasks:
            stats["total"] += 1
            if task.get("success"):
                stats["successful"] += 1
            else:
                stats["failed"] += 1
            stats["file_count"] += task.get("files_count", 0)
        
        # æ’åº
        history_tasks.sort(key=lambda x: x.get("download_time", ""), reverse=True)
        
        # åˆ†é¡µ
        start_idx = (page - 1) * size
        end_idx = start_idx + size
        paginated_tasks = history_tasks[start_idx:end_idx]
        
        return {
            "tasks": paginated_tasks,
            "stats": stats,
            "pagination": {
                "page": page,
                "size": size,
                "total": len(history_tasks),
                "pages": (len(history_tasks) + size - 1) // size
            }
        }
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºå†å²ä»»åŠ¡å¤±è´¥: {e}")
        return {
            "tasks": [],
            "stats": {"total": 0, "successful": 0, "failed": 0, "file_count": 0},
            "error": str(e)
        }

@app.get(f"{settings.app.api_prefix}/history/{{task_id}}")
async def get_history_task_detail(task_id: str) -> Dict[str, Any]:
    """è·å–å†å²ä»»åŠ¡è¯¦æƒ…"""
    try:
        # æŸ¥æ‰¾ä»»åŠ¡ç›®å½•
        task_dir = await _find_task_directory(task_id)
        if not task_dir:
            return {"error": "ä»»åŠ¡ä¸å­˜åœ¨"}
        
        # åŠ è½½ä»»åŠ¡è¯¦æƒ…
        task_detail = await _load_task_detail(task_dir)
        return task_detail
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {e}")
        return {"error": str(e)}

@app.get(f"{settings.app.api_prefix}/history/file/{{task_id}}/{{filename}}")
async def download_history_file(task_id: str, filename: str):
    """ä¸‹è½½å†å²ä»»åŠ¡æ–‡ä»¶"""
    try:
        from fastapi.responses import FileResponse
        
        # æŸ¥æ‰¾ä»»åŠ¡ç›®å½•
        task_dir = await _find_task_directory(task_id)
        if not task_dir:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        # æŸ¥æ‰¾æ–‡ä»¶
        file_path = task_dir / filename
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        return FileResponse(
            path=str(file_path),
            filename=filename,
            media_type='application/octet-stream'
        )
        
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get(f"{settings.app.api_prefix}/history/download/{{task_id}}")
async def download_task_archive(task_id: str):
    """ä¸‹è½½ä»»åŠ¡æ‰“åŒ…æ–‡ä»¶"""
    try:
        import zipfile
        import tempfile
        from fastapi.responses import FileResponse
        
        # æŸ¥æ‰¾ä»»åŠ¡ç›®å½•
        task_dir = await _find_task_directory(task_id)
        if not task_dir:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        # åˆ›å»ºä¸´æ—¶zipæ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:
            zip_path = tmp_file.name
        
        # æ‰“åŒ…ä»»åŠ¡æ–‡ä»¶
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in task_dir.rglob('*'):
                if file_path.is_file():
                    arcname = file_path.relative_to(task_dir)
                    zipf.write(file_path, arcname)
        
        return FileResponse(
            path=zip_path,
            filename=f"{task_id}.zip",
            media_type='application/zip'
        )
        
    except Exception as e:
        logger.error(f"ä¸‹è½½ä»»åŠ¡æ‰“åŒ…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post(f"{settings.app.api_prefix}/history/batch-download")
async def batch_download_tasks(request: Dict[str, List[str]]) -> Dict[str, Any]:
    """æ‰¹é‡ä¸‹è½½ä»»åŠ¡"""
    try:
        import zipfile
        import tempfile
        from fastapi.responses import FileResponse
        
        task_ids = request.get("task_ids", [])
        if not task_ids:
            raise HTTPException(status_code=400, detail="æœªæŒ‡å®šä»»åŠ¡ID")
        
        # åˆ›å»ºä¸´æ—¶zipæ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:
            zip_path = tmp_file.name
        
        successful_tasks = []
        failed_tasks = []
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for task_id in task_ids:
                try:
                    task_dir = await _find_task_directory(task_id)
                    if task_dir and task_dir.exists():
                        # æ·»åŠ ä»»åŠ¡æ–‡ä»¶åˆ°zip
                        for file_path in task_dir.rglob('*'):
                            if file_path.is_file():
                                arcname = f"{task_id}/{file_path.relative_to(task_dir)}"
                                zipf.write(file_path, arcname)
                        successful_tasks.append(task_id)
                    else:
                        failed_tasks.append(task_id)
                except Exception:
                    failed_tasks.append(task_id)
        
        return FileResponse(
            path=zip_path,
            filename=f"batch_download_{int(time.time())}.zip",
            media_type='application/zip'
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¸‹è½½å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete(f"{settings.app.api_prefix}/history/{{task_id}}")
async def delete_history_task(task_id: str) -> Dict[str, Any]:
    """åˆ é™¤å†å²ä»»åŠ¡"""
    try:
        import shutil
        
        # æŸ¥æ‰¾ä»»åŠ¡ç›®å½•
        task_dir = await _find_task_directory(task_id)
        if not task_dir:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        # åˆ é™¤ä»»åŠ¡ç›®å½•
        shutil.rmtree(task_dir)
        
        return {"message": "ä»»åŠ¡åˆ é™¤æˆåŠŸ"}
        
    except Exception as e:
        logger.error(f"åˆ é™¤ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post(f"{settings.app.api_prefix}/history/batch-delete")
async def batch_delete_tasks(request: Dict[str, List[str]]) -> Dict[str, Any]:
    """æ‰¹é‡åˆ é™¤ä»»åŠ¡"""
    try:
        import shutil
        
        task_ids = request.get("task_ids", [])
        if not task_ids:
            raise HTTPException(status_code=400, detail="æœªæŒ‡å®šä»»åŠ¡ID")
        
        successful_deletes = []
        failed_deletes = []
        
        for task_id in task_ids:
            try:
                task_dir = await _find_task_directory(task_id)
                if task_dir and task_dir.exists():
                    shutil.rmtree(task_dir)
                    successful_deletes.append(task_id)
                else:
                    failed_deletes.append(task_id)
            except Exception:
                failed_deletes.append(task_id)
        
        return {
            "message": f"æˆåŠŸåˆ é™¤ {len(successful_deletes)} ä¸ªä»»åŠ¡",
            "successful": successful_deletes,
            "failed": failed_deletes
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ é™¤å¤±è´¥: {e}")
        return {"error": str(e)}


@app.post(f"{settings.app.api_prefix}/history/{{task_id}}/ai-summary")
async def generate_task_ai_summary(task_id: str) -> Dict[str, Any]:
    """ä¸ºæŒ‡å®šä»»åŠ¡ç”ŸæˆAIæ™ºèƒ½æ€»ç»“"""
    try:
        from app.core.task_summary_generator import generate_task_summary
        
        # æŸ¥æ‰¾ä»»åŠ¡ç›®å½•
        task_dir = await _find_task_directory(task_id)
        if not task_dir or not task_dir.exists():
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
        
        logger.info(
            "å¼€å§‹ç”Ÿæˆä»»åŠ¡AIæ€»ç»“",
            task_id=task_id,
            task_dir=str(task_dir)
        )
        
        # è·å–ä»»åŠ¡æ ‡é¢˜
        task_data = await _load_task_detail(task_dir)
        task_title = task_data.get("task", {}).get("title", "æœªçŸ¥ä»»åŠ¡")
        
        # ä½¿ç”¨ä»»åŠ¡æ€»ç»“ç”Ÿæˆå™¨
        result = await generate_task_summary(task_dir, task_title, force=True)
        
        if result["success"]:
            logger.info(
                "ä»»åŠ¡AIæ€»ç»“ç”ŸæˆæˆåŠŸ",
                task_id=task_id,
                analysis_type=result.get("analysis_type", "unknown")
            )
            
            return {
                "success": True,
                "summary": result["summary"],
                "cached": False  # æ–°ç”Ÿæˆçš„æ€»ç»“
            }
        else:
            logger.warning("AIæ€»ç»“ç”Ÿæˆå¤±è´¥", task_id=task_id, error=result.get("error"))
            return {
                "success": False,
                "error": result.get("error", "æ€»ç»“ç”Ÿæˆå¤±è´¥"),
                "summary": None
            }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error("ç”ŸæˆAIæ€»ç»“å¤±è´¥", task_id=task_id, error=str(e))
        raise HTTPException(status_code=500, detail=f"ç”ŸæˆAIæ€»ç»“å¤±è´¥: {str(e)}")


@app.get(f"{settings.app.api_prefix}/history/{{task_id}}/ai-summary")
async def get_task_ai_summary(task_id: str) -> Dict[str, Any]:
    """è·å–ä»»åŠ¡çš„AIæ™ºèƒ½æ€»ç»“ï¼ˆç¼“å­˜ç‰ˆæœ¬ï¼‰"""
    try:
        from app.core.task_summary_generator import get_task_summary
        
        # æŸ¥æ‰¾ä»»åŠ¡ç›®å½•
        task_dir = await _find_task_directory(task_id)
        if not task_dir or not task_dir.exists():
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
        
        # è·å–å·²æœ‰æ€»ç»“
        summary_data = get_task_summary(task_dir)
        
        if summary_data:
            logger.info("è¿”å›ç¼“å­˜çš„AIæ€»ç»“", task_id=task_id)
            return {
                "success": True,
                "summary": summary_data,
                "cached": True  # æ¥è‡ªç¼“å­˜
            }
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œè¿”å›éœ€è¦ç”Ÿæˆçš„çŠ¶æ€
        return {
            "success": False,
            "error": "AIæ€»ç»“å°šæœªç”Ÿæˆï¼Œè¯·å…ˆè°ƒç”¨ç”Ÿæˆæ¥å£",
            "summary": None,
            "cached": False
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error("è·å–AIæ€»ç»“å¤±è´¥", task_id=task_id, error=str(e))
        raise HTTPException(status_code=500, detail=f"è·å–AIæ€»ç»“å¤±è´¥: {str(e)}")

# è¾…åŠ©å‡½æ•°
async def _detect_platform_from_metadata(task_dir: Path) -> str:
    """ä»ä»»åŠ¡å…ƒæ•°æ®ä¸­æ£€æµ‹å¹³å°ä¿¡æ¯"""
    try:
        metadata_file = task_dir / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # ä¼˜å…ˆä»downloadä¿¡æ¯ä¸­è·å–å¹³å°
            download_platform = metadata.get("download", {}).get("platform", "")
            if download_platform:
                return download_platform
            
            # ä»ä»»åŠ¡IDä¸­æ¨æ–­å¹³å°
            task_id = metadata.get("task", {}).get("id", "")
            if task_id:
                if "skywork" in task_id.lower():
                    return "skywork"
                elif "manus" in task_id.lower():
                    return "manus"
                elif "chatgpt" in task_id.lower():
                    return "chatgpt"
            
            # ä»é¡µé¢URLæ¨æ–­å¹³å°
            page_url = metadata.get("download", {}).get("page_url", "")
            if page_url:
                if "skywork.ai" in page_url:
                    return "skywork"
                elif "manus.im" in page_url or "manus.ai" in page_url:
                    return "manus"
                elif "openai.com" in page_url or "chatgpt.com" in page_url:
                    return "chatgpt"
        
        # ä»ç›®å½•åæ¨æ–­å¹³å°
        task_dir_name = task_dir.name
        if "skywork" in task_dir_name.lower():
            return "skywork"
        elif "manus" in task_dir_name.lower():
            return "manus"
        elif "chatgpt" in task_dir_name.lower():
            return "chatgpt"
        
        return "unknown"
        
    except Exception as e:
        logger.error(f"æ£€æµ‹å¹³å°ä¿¡æ¯å¤±è´¥: {e}")
        return "unknown"

async def _load_task_data(task_dir: Path, platform: str) -> Optional[Dict[str, Any]]:
    """åŠ è½½ä»»åŠ¡æ•°æ®"""
    try:
        metadata_file = task_dir / "metadata.json"
        if not metadata_file.exists():
            return None
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
        file_count = len(list(task_dir.glob('*')))
        
        # è¯»å–å†…å®¹æ–‡ä»¶
        content_file = task_dir / "content.txt"
        content_preview = ""
        if content_file.exists():
            with open(content_file, 'r', encoding='utf-8') as f:
                content = f.read()
                # æå–å†…å®¹é¢„è§ˆï¼ˆè·³è¿‡å…ƒæ•°æ®è¡Œï¼‰
                lines = content.split('\n')
                content_start = 0
                for i, line in enumerate(lines):
                    if line.startswith('=='):
                        content_start = i + 1
                        break
                if content_start < len(lines):
                    content_preview = '\n'.join(lines[content_start:content_start+3])[:200]
        
        task_info = metadata.get("task", {})
        download_info = metadata.get("download", {})
        
        title = task_info.get("title", "æœªçŸ¥ä»»åŠ¡")
        
        # è¿‡æ»¤æ— æ•ˆçš„ä»»åŠ¡æ ‡é¢˜
        if _is_invalid_task_title(title):
            return None
        
        # ğŸ”¥ å¯¹æ‰£å­ç©ºé—´çš„æ ‡é¢˜è¿›è¡Œæ™ºèƒ½æ¸…ç†
        display_title = title
        if platform == "coze_space":
            display_title = _extract_coze_smart_core(title)
            if not display_title or len(display_title) < 3:
                display_title = _clean_coze_title_core(title)
            if not display_title:
                display_title = title  # å›é€€åˆ°åŸæ ‡é¢˜
        
        return {
            "id": task_info.get("id"),
            "title": display_title,
            "platform": platform,
            "success": file_count > 1,  # å¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶è¯´æ˜ä¸‹è½½æˆåŠŸ
            "files_count": file_count,
            "content_preview": content_preview,
            "download_time": download_info.get("timestamp"),
            "download_dir": str(task_dir),
            "task_date": task_info.get("date"),
            "task_url": task_info.get("url"),
            "page_url": download_info.get("page_url"),
            "page_title": download_info.get("page_title"),
            "content_length": download_info.get("content_length", 0)
        }
        
    except Exception as e:
        logger.error(f"åŠ è½½ä»»åŠ¡æ•°æ®å¤±è´¥: {e}")
        return None

async def _find_task_directory(task_id: str) -> Optional[Path]:
    """æŸ¥æ‰¾ä»»åŠ¡ç›®å½•"""
    try:
        # ğŸ”¥ æ‰©å±•æœç´¢èŒƒå›´ï¼šåŒ…å«å„å¹³å°ç‰¹å®šçš„ä¸‹è½½ç›®å½•
        search_dirs = [
            "data/history_downloads",
            "data/multi_platform_downloads",
            "data/skywork_history", 
            "data/skywork_downloads",
            "data/manus_history",
            "data/manus_downloads", 
            "data/coze_space_history_downloads",
            "data/coze_downloads"
        ]
        
        # åœ¨å¤šå¹³å°ä¸‹è½½ç›®å½•ä¸­æŸ¥æ‰¾
        multi_platform_dir = Path("data/multi_platform_downloads")
        if multi_platform_dir.exists():
            for session_dir in multi_platform_dir.glob("multi_platform_history_*"):
                for platform_dir in session_dir.iterdir():
                    if platform_dir.is_dir():
                        task_dir = platform_dir / f"task_{task_id}"
                        if task_dir.exists():
                            return task_dir
        
        # åœ¨å„å¹³å°ç‰¹å®šç›®å½•ä¸­æŸ¥æ‰¾
        for dir_path in search_dirs:
            base_dir = Path(dir_path)
            if base_dir.exists():
                # ç›´æ¥æŸ¥æ‰¾ä»»åŠ¡ç›®å½•
                task_dir = base_dir / f"task_{task_id}"
                if task_dir.exists():
                    return task_dir
                
                # åœ¨ä¸‹è½½ä¼šè¯ç›®å½•ä¸­æŸ¥æ‰¾
                for session_dir in base_dir.iterdir():
                    if session_dir.is_dir():
                        task_dir = session_dir / f"task_{task_id}"
                        if task_dir.exists():
                            return task_dir
                        
                        # è¿˜å¯èƒ½æ˜¯ç›´æ¥åŒ…å«ä»»åŠ¡IDçš„ç›®å½•
                        if task_id in session_dir.name:
                            return session_dir
        
        return None
        
    except Exception as e:
        logger.error(f"æŸ¥æ‰¾ä»»åŠ¡ç›®å½•å¤±è´¥: {e}")
        return None

async def _load_task_detail(task_dir: Path) -> Dict[str, Any]:
    """åŠ è½½ä»»åŠ¡è¯¦ç»†ä¿¡æ¯"""
    try:
        # åŠ è½½å…ƒæ•°æ®
        metadata_file = task_dir / "metadata.json"
        metadata = {}
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        
        # æ‰«ææ‰€æœ‰æ–‡ä»¶
        files = []
        for file_path in task_dir.glob('*'):
            if file_path.is_file():
                file_info = {
                    "name": file_path.name,
                    "size": file_path.stat().st_size,
                    "type": _get_file_type(file_path),
                }
                
                # å¦‚æœæ˜¯æ–‡æœ¬æ–‡ä»¶ï¼Œè¯»å–å†…å®¹
                if file_info["type"] in ["text", "json", "html"]:
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            file_info["content"] = content
                    except:
                        file_info["content"] = "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹"
                
                files.append(file_info)
        
        return {
            "task": metadata.get("task", {}),
            "download": metadata.get("download", {}),
            "files": files,
            "task_dir": str(task_dir)
        }
        
    except Exception as e:
        logger.error(f"åŠ è½½ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {e}")
        return {"error": str(e)}

def _get_file_type(file_path: Path) -> str:
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šæ–‡ä»¶ç±»å‹"""
    suffix = file_path.suffix.lower()
    
    type_map = {
        '.txt': 'text',
        '.json': 'json',
        '.html': 'html',
        '.htm': 'html',
        '.png': 'image',
        '.jpg': 'image',
        '.jpeg': 'image',
        '.gif': 'image',
        '.pdf': 'pdf',
        '.doc': 'document',
        '.docx': 'document',
        '.xls': 'spreadsheet',
        '.xlsx': 'spreadsheet'
    }
    
    return type_map.get(suffix, 'binary')

async def _load_tasks_from_download_report(report_path: Path, platform: str) -> List[Dict[str, Any]]:
    """ä»ä¸‹è½½æŠ¥å‘Šä¸­åŠ è½½ä»»åŠ¡æ•°æ®"""
    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            report_data = json.load(f)
        
        tasks = []
        results = report_data.get("results", [])
        download_time = report_data.get("download_time", "")
        
        # ç”¨äºæ‰£å­ç©ºé—´æ™ºèƒ½å»é‡çš„é›†åˆ
        seen_smart_cores = set()
        
        for result in results:
            try:
                # åŸºæœ¬ä»»åŠ¡ä¿¡æ¯
                task_id = result.get("task_id", "")
                title = result.get("title", "æœªçŸ¥ä»»åŠ¡")
                
                # ğŸ”¥ æ‰£å­ç©ºé—´ä¸“ç”¨æ™ºèƒ½å»é‡
                if platform == "coze_space":
                    smart_core = _extract_coze_smart_core(title)
                    if smart_core in seen_smart_cores:
                        logger.debug(f"è·³è¿‡é‡å¤ä»»åŠ¡: {title[:60]}...")
                        continue
                    seen_smart_cores.add(smart_core)
                else:
                    # å…¶ä»–å¹³å°çš„åŸæœ‰å»é‡é€»è¾‘
                    clean_title = _clean_task_title_for_dedup(title)
                    if clean_title in seen_smart_cores:
                        logger.debug(f"è·³è¿‡é‡å¤ä»»åŠ¡: {title}")
                        continue
                    seen_smart_cores.add(clean_title)
                
                # ğŸ”¥ å¯¹æ‰£å­ç©ºé—´çš„æ ‡é¢˜è¿›è¡Œæ™ºèƒ½æ¸…ç†
                display_title = title
                if platform == "coze_space":
                    display_title = _extract_coze_smart_core(title)
                    if not display_title or len(display_title) < 3:
                        display_title = _clean_coze_title_core(title)
                    if not display_title:
                        display_title = title  # å›é€€åˆ°åŸæ ‡é¢˜
                
                task_data = {
                    "id": task_id,
                    "title": display_title,
                    "platform": platform,
                    "success": result.get("success", False),
                    "download_time": download_time,
                    "files_count": len(result.get("files", [])),
                    "download_dir": result.get("download_dir", ""),
                    "content_preview": display_title[:200] + "..." if len(display_title) > 200 else display_title
                }
                
                # å¦‚æœæœ‰ä¸‹è½½ç›®å½•ï¼Œå°è¯•è·å–æ›´å¤šä¿¡æ¯
                if task_data["download_dir"]:
                    task_dir = Path(task_data["download_dir"])
                    if task_dir.exists():
                        # æŸ¥æ‰¾å¹¶è¯»å–metadata
                        metadata_file = task_dir / "metadata.json"
                        if metadata_file.exists():
                            try:
                                with open(metadata_file, 'r', encoding='utf-8') as f:
                                    metadata = json.load(f)
                                    
                                # æ”¯æŒæ–°çš„æ ‡å‡†åŒ–æ ¼å¼
                                if "task" in metadata:
                                    task_info = metadata["task"]
                                    download_info = metadata.get("download", {})
                                    
                                    task_data.update({
                                        "task_url": task_info.get("url", ""),
                                        "task_date": task_info.get("date", ""),
                                        "page_title": task_info.get("title", task_data["title"]),
                                        "page_url": task_info.get("url", ""),
                                        "content_length": len(task_info.get("preview", "")),
                                        "timestamp": download_info.get("timestamp", download_time)
                                    })
                                else:
                                    # å…¼å®¹æ—§æ ¼å¼
                                    task_data.update({
                                        "task_url": metadata.get("url", ""),
                                        "page_title": metadata.get("title", task_data["title"]),
                                        "page_url": metadata.get("url", ""),
                                        "content_length": len(metadata.get("preview", "")),
                                        "timestamp": metadata.get("timestamp", download_time)
                                    })
                                    
                            except Exception as e:
                                logger.warning(f"è¯»å–metadataå¤±è´¥: {e}")
                
                # æœ€ç»ˆæ£€æŸ¥ä»»åŠ¡æ˜¯å¦æœ‰æ•ˆ
                if not _is_invalid_task_title(task_data["title"]):
                    tasks.append(task_data)
                
            except Exception as e:
                logger.warning(f"è§£æä»»åŠ¡ç»“æœå¤±è´¥: {e}")
                continue
        
        logger.info(f"ä»ä¸‹è½½æŠ¥å‘ŠåŠ è½½äº† {len(tasks)} ä¸ªæœ‰æ•ˆä»»åŠ¡ï¼ˆå»é‡åï¼‰")
        return tasks
        
    except Exception as e:
        logger.error(f"è¯»å–ä¸‹è½½æŠ¥å‘Šå¤±è´¥: {e}")
        return []

def _deduplicate_tasks(tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """å»é™¤é‡å¤ä»»åŠ¡"""
    if not tasks:
        return tasks
    
    # ç”¨äºè·Ÿè¸ªå·²è§è¿‡çš„ä»»åŠ¡
    seen_tasks = {}
    deduplicated = []
    
    for task in tasks:
        # ç”Ÿæˆå»é‡é”®
        dedup_key = _generate_dedup_key(task)
        
        if dedup_key not in seen_tasks:
            # è¿™æ˜¯ä¸€ä¸ªæ–°ä»»åŠ¡
            seen_tasks[dedup_key] = task
            deduplicated.append(task)
        else:
            # è¿™æ˜¯é‡å¤ä»»åŠ¡ï¼Œé€‰æ‹©æ›´å¥½çš„ç‰ˆæœ¬
            existing_task = seen_tasks[dedup_key]
            better_task = _choose_better_task(existing_task, task)
            
            if better_task != existing_task:
                # æ›¿æ¢ä¸ºæ›´å¥½çš„ä»»åŠ¡
                seen_tasks[dedup_key] = better_task
                # åœ¨åˆ—è¡¨ä¸­æ‰¾åˆ°å¹¶æ›¿æ¢
                for i, t in enumerate(deduplicated):
                    if t == existing_task:
                        deduplicated[i] = better_task
                        break
    
    return deduplicated

def _generate_dedup_key(task: Dict[str, Any]) -> str:
    """ç”Ÿæˆä»»åŠ¡å»é‡é”®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    platform = task.get("platform", "unknown")
    task_id = task.get("id", "")
    title = task.get("title", "").strip()
    download_time = task.get("download_time", "")
    
    # ğŸ”¥ æ‰£å­ç©ºé—´ä¸“ç”¨çš„å¼ºåŒ–å»é‡é€»è¾‘
    if platform == "coze_space":
        return _generate_coze_dedup_key(task)
    
    # å…¶ä»–å¹³å°çš„åŸæœ‰é€»è¾‘
    # 1. åŸºäºä»»åŠ¡IDè¿›è¡Œå»é‡ï¼ˆæœ€ç²¾ç¡®çš„æ ‡è¯†ï¼‰
    if task_id and task_id.strip() and not _is_auto_generated_id(task_id, platform):
        return f"{platform}:id:{task_id}"
    
    # 2. åŸºäºé¡µé¢URLè¿›è¡Œå»é‡
    page_url = task.get("page_url", "")
    if page_url and page_url not in ["https://manus.im/app", "https://space.coze.cn/", ""]:
        return f"{platform}:url:{page_url}"
    
    # 3. åŸºäºæ ‡é¢˜è¿›è¡Œå»é‡
    clean_title = _clean_title_for_dedup(title)
    if clean_title and len(clean_title) >= 5:
        time_part = download_time[:16] if download_time else "no_time"
        return f"{platform}:title:{clean_title}:time:{time_part}"
    
    # 4. æœ€åå¤‡ç”¨æ–¹æ¡ˆ
    content_length = task.get("content_length", 0)
    time_part = download_time[:16] if download_time else "no_time"
    return f"{platform}:fallback:{title[:20]}:length:{content_length}:time:{time_part}"

def _generate_coze_dedup_key(task: Dict[str, Any]) -> str:
    """æ‰£å­ç©ºé—´ä¸“ç”¨å»é‡é”®ç”Ÿæˆ"""
    title = task.get("title", "").strip()
    task_id = task.get("id", "")
    content_preview = task.get("content_preview", "")
    
    # ğŸ”¥ æ™ºèƒ½æ ‡é¢˜æ¸…ç†ï¼šå¤„ç†è¶…é•¿æ ‡é¢˜å’Œå¤šä»»åŠ¡åˆå¹¶çš„æƒ…å†µ
    clean_core = _extract_coze_smart_core(title)
    
    # ğŸ”¥ ä½¿ç”¨å†…å®¹ç‰¹å¾è¾…åŠ©å»é‡
    content_hash = ""
    if content_preview and len(content_preview) > 20:
        import hashlib
        content_hash = hashlib.md5(content_preview.encode('utf-8')).hexdigest()[:8]
    
    # ğŸ”¥ ç»„åˆå»é‡é”®ï¼šæ ¸å¿ƒæ ‡é¢˜ + å†…å®¹å“ˆå¸Œ
    if clean_core and content_hash:
        return f"coze_space:smart:{clean_core}:{content_hash}"
    elif clean_core:
        return f"coze_space:smart:{clean_core}"
    else:
        # å¤‡ç”¨æ–¹æ¡ˆ
        session_id = _extract_session_id_from_task_id(task_id)
        return f"coze_space:fallback:{title[:30]}:session:{session_id}"

def _extract_coze_smart_core(title: str) -> str:
    """æ™ºèƒ½æå–æ‰£å­ç©ºé—´æ ‡é¢˜æ ¸å¿ƒ"""
    if not title:
        return ""
    
    import re
    
    # ğŸ”¥ å¤„ç†è¶…é•¿æ ‡é¢˜ï¼šå¯èƒ½æ˜¯å¤šä¸ªä»»åŠ¡æ‹¼æ¥çš„
    if len(title) > 80:
        # æ›´ç²¾ç¡®çš„æ¨¡å¼åŒ¹é…ï¼Œä¼˜å…ˆæå–ç¬¬ä¸€ä¸ªå®Œæ•´ä¸»é¢˜
        patterns = [
            # åŒ¹é… "è¿‡å»Nå¤© ä¸»é¢˜ çŠ¶æ€æ ‡è®°" æ ¼å¼
            r'(?:è¿‡å»\d+å¤©\s+)?([^ä¸€è½®ä»»åŠ¡å®Œæˆä»»åŠ¡å·²ç»“æŸ]{4,40}?)(?:\s+ä¸€è½®ä»»åŠ¡å®Œæˆ|\s+ä»»åŠ¡å·²ç»“æŸ)',
            # åŒ¹é… "è¿‡å¾€ ä¸»é¢˜ çŠ¶æ€æ ‡è®°" æ ¼å¼
            r'(?:è¿‡å¾€\s+)?([^ä¸€è½®ä»»åŠ¡å®Œæˆä»»åŠ¡å·²ç»“æŸ]{4,40}?)(?:\s+ä¸€è½®ä»»åŠ¡å®Œæˆ|\s+ä»»åŠ¡å·²ç»“æŸ)',
            # åŒ¹é…å¼€å¤´çš„ä¸»é¢˜ï¼ˆç›´åˆ°ç¬¬ä¸€ä¸ªçŠ¶æ€è¯æˆ–æ—¶é—´å‰ç¼€ï¼‰
            r'^([^ä¸€è½®ä»»åŠ¡å®Œæˆä»»åŠ¡å·²ç»“æŸ\s]{4,40}?)(?:\s+è¿‡å»\d+å¤©|\s+è¿‡å¾€|\s+ä¸€è½®ä»»åŠ¡å®Œæˆ|\s+ä»»åŠ¡å·²ç»“æŸ)',
            # åŒ¹é…å¼€å¤´åˆ°ç¬¬ä¸€ä¸ªç©ºæ ¼çš„ä¸»é¢˜
            r'^([^\s]{3,30}?)(?=\s)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, title)
            if matches:
                core = matches[0].strip()
                
                # æ¸…ç†æå–çš„æ ¸å¿ƒå†…å®¹
                # ç§»é™¤æ—¶é—´å‰ç¼€
                time_prefixes = ['è¿‡å»', 'è¿‡å¾€', 'æ–°ä»»åŠ¡', 'æœ€è¿‘']
                for prefix in time_prefixes:
                    if core.startswith(prefix):
                        # æ‰¾åˆ°æ•°å­—åçš„éƒ¨åˆ†
                        remaining = re.sub(r'^' + prefix + r'\d*[å¤©æœˆ]?\s*', '', core)
                        if len(remaining) >= 3:
                            core = remaining
                        break
                
                # è¿›ä¸€æ­¥æ¸…ç†
                core = _clean_coze_title_core(core)
                if len(core) >= 3:
                    return core[:40]
    
    # ğŸ”¥ å¤„ç†æ­£å¸¸é•¿åº¦æ ‡é¢˜ - ç›´æ¥æ¸…ç†
    clean = _clean_coze_title_core(title)
    
    # ğŸ”¥ å¦‚æœæ¸…ç†åä»ç„¶å¾ˆé•¿ï¼ŒæŒ‰æ„ä¹‰å•å…ƒæˆªå–
    if len(clean) > 40:
        # æŒ‰å¸¸è§åˆ†éš”ç¬¦å’Œè¯­ä¹‰å•å…ƒåˆ†å‰²
        parts = re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]+', clean)
        
        # é€‰æ‹©æœ€æœ‰æ„ä¹‰çš„å‰å‡ ä¸ªéƒ¨åˆ†
        meaningful_parts = []
        total_length = 0
        
        for part in parts:
            part = part.strip()
            if len(part) >= 2:  # è‡³å°‘2ä¸ªå­—ç¬¦
                # è¿‡æ»¤æ‰æ˜æ˜¾çš„å™ªéŸ³è¯æ±‡
                if part not in ['è¿‡å»', 'è¿‡å¾€', 'æ–°ä»»åŠ¡', 'æœ€è¿‘', 'å¤©', 'ä¸ªæœˆ', 'å¹´', 'ä¸€è½®', 'ä»»åŠ¡', 'å®Œæˆ', 'å·²ç»“æŸ']:
                    if total_length + len(part) <= 35:  # æ§åˆ¶æ€»é•¿åº¦
                        meaningful_parts.append(part)
                        total_length += len(part)
                    else:
                        break
        
        if meaningful_parts:
            clean = ' '.join(meaningful_parts)
    
    return clean[:40] if clean else ""

def _clean_coze_title_core(title: str) -> str:
    """æ¸…ç†æ‰£å­ç©ºé—´æ ‡é¢˜æ ¸å¿ƒ"""
    if not title:
        return ""
    
    clean = title.strip()
    
    # ç§»é™¤çŠ¶æ€æ ‡è®°
    status_markers = [
        "ä¸€è½®ä»»åŠ¡å®Œæˆ", "ä»»åŠ¡å·²ç»“æŸ", "ä»»åŠ¡å·²å®Œæˆ", 
        "ä»»åŠ¡å®Œæˆ", "ä¸‹è½½å®Œæˆ", "å¤„ç†å®Œæˆ"
    ]
    for marker in status_markers:
        clean = clean.replace(marker, "")
    
    # æ”¹è¿›çš„æ—¶é—´å‰ç¼€ç§»é™¤ - æ”¯æŒå¤šç§æ¨¡å¼
    import re
    
    # ğŸ”¥ æ›´å¼ºåŠ›çš„æ—¶é—´å‰ç¼€æ¸…ç†
    # ç§»é™¤å¼€å¤´çš„æ—¶é—´å‰ç¼€ï¼ˆæ›´ç²¾ç¡®çš„åŒ¹é…ï¼‰
    time_patterns = [
        r'^è¿‡å»\d+å¤©\s*',      # è¿‡å»7å¤©ã€è¿‡å»30å¤©
        r'^è¿‡å¾€\s*',          # è¿‡å¾€
        r'^æ–°ä»»åŠ¡\s*',        # æ–°ä»»åŠ¡  
        r'^æœ€è¿‘\s*',          # æœ€è¿‘
        r'^å†å²\s*',          # å†å²
        r'^è¿‡å»\d+ä¸ªæœˆ\s*',    # è¿‡å»3ä¸ªæœˆ
        r'^ä¸Šä¸ªæœˆ\s*',        # ä¸Šä¸ªæœˆ
        r'^æœ¬æœˆ\s*',          # æœ¬æœˆ
    ]
    
    for pattern in time_patterns:
        clean = re.sub(pattern, '', clean, flags=re.IGNORECASE)
    
    # ç§»é™¤ä¸­é—´å‡ºç°çš„æ—¶é—´å‰ç¼€ï¼ˆå¯¹äºè¿æ¥çš„æ ‡é¢˜ï¼‰
    middle_time_patterns = [
        r'\s+è¿‡å»\d+å¤©\s+',
        r'\s+è¿‡å»\d+ä¸ªæœˆ\s+',
        r'\s+è¿‡å¾€\s+',
        r'\s+æ–°ä»»åŠ¡\s+',
        r'\s+æœ€è¿‘\s+',
        r'\s+ä¸Šä¸ªæœˆ\s+',
        r'\s+æœ¬æœˆ\s+',
    ]
    
    for pattern in middle_time_patterns:
        clean = re.sub(pattern, ' ', clean, flags=re.IGNORECASE)
    
    # ğŸ”¥ å†æ¬¡ç§»é™¤å¼€å¤´çš„æ—¶é—´å‰ç¼€ï¼ˆå¤„ç†æ¸…ç†åéœ²å‡ºçš„å‰ç¼€ï¼‰
    for pattern in time_patterns:
        clean = re.sub(pattern, '', clean, flags=re.IGNORECASE)
    
    # ç§»é™¤AIå›å¤æ ‡è¯†
    ai_prefixes = ["æˆ‘å·²å®Œæˆ", "æˆ‘å·²ä¸ºæ‚¨", "æ„Ÿè°¢æ‚¨çš„åé¦ˆ", "æ ¹æ®æ‚¨çš„è¦æ±‚"]
    for prefix in ai_prefixes:
        if clean.startswith(prefix):
            clean = clean[len(prefix):].strip()
    
    # æ¸…ç†å¤šä½™çš„ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·
    clean = re.sub(r'\s+', ' ', clean).strip()
    clean = clean.strip("ï¼Œã€‚ï¼ï¼Ÿã€ ï¼šï¼›")
    
    # ğŸ”¥ å¦‚æœæ¸…ç†åä»ç„¶å¾ˆé•¿ï¼Œæˆªå–å‰é¢çš„æœ‰æ„ä¹‰éƒ¨åˆ†
    if len(clean) > 50:
        # æŒ‰å¸¸è§åˆ†éš”ç¬¦åˆ†å‰²ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„ä¸»é¢˜
        parts = re.split(r'[ï¼Œã€‚ï¼›ï¼š\s]+', clean)
        meaningful_parts = []
        total_length = 0
        
        for part in parts:
            if len(part) >= 2:  # è‡³å°‘2ä¸ªå­—ç¬¦çš„æœ‰æ„ä¹‰éƒ¨åˆ†
                if total_length + len(part) <= 40:  # æ§åˆ¶æ€»é•¿åº¦
                    meaningful_parts.append(part)
                    total_length += len(part)
                else:
                    break
        
        if meaningful_parts:
            clean = ' '.join(meaningful_parts)
    
    # å¦‚æœæ¸…ç†åå¤ªçŸ­ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„ç‰‡æ®µ
    if len(clean) < 3 and title:
        # ä»åŸæ ‡é¢˜ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„è¯ç»„
        words = re.split(r'[^\w\u4e00-\u9fff]+', title)
        meaningful_words = [w for w in words if len(w) >= 2 and w not in ['è¿‡å»', 'å¤©', 'è¿‡å¾€', 'æ–°ä»»åŠ¡', 'æœ€è¿‘', 'ä¸€è½®', 'ä»»åŠ¡', 'å®Œæˆ', 'å·²ç»“æŸ', 'ä¸ªæœˆ']]
        if meaningful_words:
            clean = ' '.join(meaningful_words[:3])  # å–å‰3ä¸ªæœ‰æ„ä¹‰çš„è¯
    
    return clean

def _is_auto_generated_id(task_id: str, platform: str) -> bool:
    """æ£€æŸ¥ä»»åŠ¡IDæ˜¯å¦æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ç´¢å¼•"""
    if platform == "coze_space":
        # æ‰£å­ç©ºé—´çš„IDæ ¼å¼: coze_space_history_N_timestamp
        import re
        pattern = r"coze_space_history_\d+_\d+"
        return bool(re.match(pattern, task_id))
    return False

def _clean_task_title_for_dedup(title: str) -> str:
    """æ¸…ç†ä»»åŠ¡æ ‡é¢˜ç”¨äºå»é‡æ¯”è¾ƒ"""
    if not title:
        return ""
    
    # ç§»é™¤çŠ¶æ€å…³é”®è¯
    status_words = ["ä¸€è½®ä»»åŠ¡å®Œæˆ", "ä»»åŠ¡å·²ç»“æŸ", "ä»»åŠ¡å·²å®Œæˆ", "ä¸‹è½½å¤±è´¥"]
    clean = title
    for word in status_words:
        clean = clean.replace(word, "")
    
    # ç§»é™¤æ—¶é—´å‰ç¼€
    time_prefixes = ["è¿‡å»7å¤©", "è¿‡å»30å¤©", "è¿‡å¾€", "æ–°ä»»åŠ¡"]
    for prefix in time_prefixes:
        clean = clean.replace(prefix, "")
    
    # æ¸…ç†ç©ºæ ¼å¹¶è½¬æ¢ä¸ºå°å†™
    return " ".join(clean.split()).strip().lower()

def _is_invalid_task_title(title: str) -> bool:
    """æ£€æŸ¥ä»»åŠ¡æ ‡é¢˜æ˜¯å¦æ— æ•ˆ"""
    if not title or title.strip() == "":
        return True
    
    # æ£€æŸ¥æ˜¯å¦åªåŒ…å«çŠ¶æ€ä¿¡æ¯
    status_only_patterns = ["æœªçŸ¥ä»»åŠ¡", "ä¸‹è½½å¤±è´¥", "æ— æ ‡é¢˜", "æ— æ•ˆä»»åŠ¡"]
    clean_title = title.strip().lower()
    
    return any(pattern in clean_title for pattern in status_only_patterns)

def _extract_session_id_from_task_id(task_id: str) -> str:
    """ä»ä»»åŠ¡IDä¸­æå–ä¼šè¯ID"""
    if "_" in task_id:
        # å¯¹äºæ ¼å¼å¦‚ coze_space_history_0_1748783734ï¼Œæå–æœ€åçš„æ—¶é—´æˆ³éƒ¨åˆ†
        parts = task_id.split("_")
        if len(parts) >= 2:
            return parts[-1]  # è¿”å›æ—¶é—´æˆ³éƒ¨åˆ†
    return "unknown"

def _extract_coze_core_content(title: str) -> str:
    """æå–æ‰£å­ç©ºé—´æ ‡é¢˜çš„æ ¸å¿ƒå†…å®¹"""
    if not title:
        return ""
    
    # ç§»é™¤æ‰£å­ç©ºé—´ç‰¹æœ‰çš„å‰ç¼€å’Œåç¼€
    content = title
    
    # ç§»é™¤å¸¸è§çš„å‰ç¼€
    prefixes_to_remove = [
        "æ–°ä»»åŠ¡ ",
        "è¿‡å»7å¤© ",
        "è¿‡å»30å¤© ",
        "è¿‡å¾€ "
    ]
    
    for prefix in prefixes_to_remove:
        if content.startswith(prefix):
            content = content[len(prefix):]
    
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„ä¸»é¢˜
    # æŒ‰ç©ºæ ¼æˆ–å¸¸è§åˆ†éš”ç¬¦åˆ†å‰²
    import re
    
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®è´¨æ€§çš„ä¸»é¢˜ï¼ˆé€šå¸¸åœ¨å‰50ä¸ªå­—ç¬¦å†…ï¼‰
    meaningful_parts = []
    parts = re.split(r'[ï¼Œã€‚\s]+', content)
    
    for part in parts:
        if len(part) >= 3 and not part in ["ä¸€è½®ä»»åŠ¡å®Œæˆ", "ä»»åŠ¡å·²ç»“æŸ", "æœ"]:
            meaningful_parts.append(part)
        if len(meaningful_parts) >= 2:  # å–å‰ä¸¤ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†
            break
    
    if meaningful_parts:
        core = " ".join(meaningful_parts[:2])  # å–å‰ä¸¤ä¸ªéƒ¨åˆ†
        # é™åˆ¶é•¿åº¦
        return core[:50].strip()
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼Œè¿”å›å‰30ä¸ªå­—ç¬¦
    return content[:30].strip()

def _clean_title_for_dedup(title: str) -> str:
    """æ¸…ç†æ ‡é¢˜ç”¨äºå»é‡"""
    if not title:
        return ""
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯é€šç”¨é—®å€™è¯­æˆ–æ— æ„ä¹‰æ ‡é¢˜
    greeting_patterns = [
        "ä½ å¥½ï¼Œ",
        "æ‚¨å¥½ï¼Œ", 
        "æˆ‘èƒ½ä¸ºä½ åšä»€ä¹ˆ",
        "æˆ‘èƒ½ä¸ºæ‚¨åšä»€ä¹ˆ",
        "æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©",
        "è¯·é—®éœ€è¦ä»€ä¹ˆå¸®åŠ©",
        "æ„Ÿè°¢æ‚¨ä½¿ç”¨"
    ]
    
    for pattern in greeting_patterns:
        if pattern in title:
            return ""  # è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºè¿™æ˜¯æ— æ•ˆæ ‡é¢˜
    
    # ç§»é™¤å¸¸è§çš„AIå›å¤å‰ç¼€
    prefixes_to_remove = [
        "æˆ‘å·²å®Œæˆå¯¹",
        "æˆ‘å·²ä¸ºæ‚¨",
        "æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼",
        "ä½ å¥½ï¼Œ",
        "æ‚¨å¥½ï¼Œ",
        "æˆ‘æ˜ç™½äº†ï¼Œ"
    ]
    
    clean_title = title
    for prefix in prefixes_to_remove:
        if clean_title.startswith(prefix):
            # å°è¯•æå–çœŸæ­£çš„ä¸»é¢˜
            remaining = clean_title[len(prefix):].strip()
            
            # æŸ¥æ‰¾æ ¸å¿ƒä¸»é¢˜çš„ç»“æŸç‚¹
            end_patterns = [
                "çš„å…¨é¢åˆ†æ",
                "çš„è¯¦ç»†åˆ†æ", 
                "çš„åˆ†ææŠ¥å‘Š",
                "çš„ç ”ç©¶æŠ¥å‘Š",
                "åˆ†ææŠ¥å‘Š",
                "åŠŸèƒ½ç‰¹ç‚¹",
                "æœ€æ–°ç‰ˆæœ¬",
                "ã€‚",
                "ï¼Œ"
            ]
            
            for pattern in end_patterns:
                if pattern in remaining:
                    pattern_index = remaining.find(pattern)
                    if 0 < pattern_index < 50:  # åˆç†çš„é•¿åº¦èŒƒå›´
                        clean_title = remaining[:pattern_index].strip()
                        break
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°ç»“æŸæ¨¡å¼ï¼Œå–å‰30ä¸ªå­—ç¬¦
                clean_title = remaining[:30].strip()
            break
    
    # è¿›ä¸€æ­¥æ¸…ç†æ ‡é¢˜
    clean_title = clean_title.strip("ï¼Œã€‚ï¼ï¼Ÿã€ ")
    
    # ç§»é™¤ç‰ˆæœ¬å·å’Œæ—¶é—´ä¿¡æ¯çš„å½±å“
    import re
    clean_title = re.sub(r'\s+R\d+.*$', '', clean_title)  # ç§»é™¤ R1, R2 ç­‰ç‰ˆæœ¬å·
    clean_title = re.sub(r'\s+\d{4}å¹´.*$', '', clean_title)  # ç§»é™¤å¹´ä»½ä¿¡æ¯
    clean_title = re.sub(r'\s+æœ€æ–°.*$', '', clean_title)  # ç§»é™¤"æœ€æ–°"ç›¸å…³åç¼€
    
    return clean_title.strip()

def _choose_better_task(task1: Dict[str, Any], task2: Dict[str, Any]) -> Dict[str, Any]:
    """åœ¨ä¸¤ä¸ªé‡å¤ä»»åŠ¡ä¸­é€‰æ‹©æ›´å¥½çš„ä¸€ä¸ª"""
    
    # ä¼˜å…ˆé€‰æ‹©æ ‡é¢˜æ›´ç®€æ´çš„ä»»åŠ¡
    title1 = task1.get("title", "")
    title2 = task2.get("title", "")
    
    # è®¡ç®—æ ‡é¢˜è´¨é‡åˆ†æ•°ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    score1 = _calculate_title_quality_score(title1)
    score2 = _calculate_title_quality_score(title2)
    
    if score1 != score2:
        return task1 if score1 < score2 else task2
    
    # å¦‚æœæ ‡é¢˜è´¨é‡ç›¸åŒï¼Œä¼˜å…ˆé€‰æ‹©ä¸‹è½½æ—¶é—´æ›´æ™šçš„ï¼ˆæ›´æ–°çš„ç‰ˆæœ¬ï¼‰
    time1 = task1.get("download_time", "")
    time2 = task2.get("download_time", "")
    
    if time1 and time2:
        return task2 if time2 > time1 else task1
    
    # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª
    return task1

def _calculate_title_quality_score(title: str) -> int:
    """è®¡ç®—æ ‡é¢˜è´¨é‡åˆ†æ•°ï¼ˆè¶Šä½è¶Šå¥½ï¼‰"""
    if not title:
        return 1000
    
    score = 0
    
    # é•¿åº¦æƒ©ç½šï¼šè¿‡é•¿çš„æ ‡é¢˜å¾—åˆ†æ›´é«˜
    if len(title) > 30:
        score += (len(title) - 30) * 2
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«AIå›å¤ç‰¹å¾
    ai_phrases = [
        "æˆ‘å·²å®Œæˆ", "æˆ‘å·²ä¸ºæ‚¨", "æ„Ÿè°¢æ‚¨çš„åé¦ˆ", "æˆ‘æ˜ç™½äº†",
        "æ ¹æ®æ‚¨çš„è¦æ±‚", "ä¸ºæ‚¨æä¾›", "åˆ†ææŠ¥å‘Š", "è¯¦ç»†çš„"
    ]
    
    for phrase in ai_phrases:
        if phrase in title:
            score += 50  # é‡å¤§æƒ©ç½š
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šçš„æ ‡ç‚¹ç¬¦å·
    punct_count = sum(1 for c in title if c in "ï¼Œã€‚ï¼ï¼Ÿã€")
    if punct_count > 3:
        score += punct_count * 10
    
    # å¥–åŠ±ç®€æ´æ˜ç¡®çš„æ ‡é¢˜
    if 5 <= len(title) <= 25 and not any(phrase in title for phrase in ai_phrases):
        score -= 20
    
    return score

def _is_invalid_task_title(title: str) -> bool:
    """åˆ¤æ–­ä»»åŠ¡æ ‡é¢˜æ˜¯å¦æ— æ•ˆ"""
    if not title or title.strip() == "":
        return True
    
    # è¿‡æ»¤è¿‡é•¿çš„æ ‡é¢˜ï¼ˆè¶…è¿‡100ä¸ªå­—ç¬¦çš„å¾ˆå¯èƒ½æ˜¯å›å¤å†…å®¹ï¼‰
    if len(title) > 100:
        return True
    
    # è¿‡æ»¤é—®å€™è¯­
    greeting_patterns = [
        "ä½ å¥½ï¼Œ",
        "æ‚¨å¥½ï¼Œ", 
        "æˆ‘èƒ½ä¸ºä½ åšä»€ä¹ˆ",
        "æˆ‘èƒ½ä¸ºæ‚¨åšä»€ä¹ˆ",
        "æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©",
        "è¯·é—®éœ€è¦ä»€ä¹ˆå¸®åŠ©",
        "æ„Ÿè°¢æ‚¨ä½¿ç”¨"
    ]
    
    for pattern in greeting_patterns:
        if pattern in title:
            return True
    
    # è¿‡æ»¤æ˜æ˜¾çš„AIå›å¤å†…å®¹
    ai_reply_patterns = [
        "æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼",
        "æˆ‘æ˜ç™½äº†ï¼Œæ‚¨å¸Œæœ›",
        "æ ¹æ®æ‚¨çš„è¦æ±‚",
        "æˆ‘ä¼šä¸ºæ‚¨",
        "æˆ‘å°†ä¸ºæ‚¨",
        "ç°åœ¨ä¸ºæ‚¨æä¾›",
        "æˆ‘å·²ç»ä¸ºæ‚¨",
        "ç¨åæˆ‘ä¼š"
    ]
    
    for pattern in ai_reply_patterns:
        if pattern in title:
            return True
    
    # è¿‡æ»¤åŒ…å«å¤ªå¤šæ ‡ç‚¹ç¬¦å·çš„æ ‡é¢˜
    punct_count = sum(1 for c in title if c in "ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘")
    if punct_count > len(title) * 0.2:  # æ ‡ç‚¹ç¬¦å·è¶…è¿‡20%
        return True
    
    # è¿‡æ»¤åªåŒ…å«ç‰¹æ®Šå­—ç¬¦æˆ–æ•°å­—çš„æ ‡é¢˜
    if title.replace(' ', '').replace('\n', '').replace('\t', '') == "":
        return True
    
    return False



if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "app.api.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    ) 